# Gradient_Descent

The underlying principle used is that of a straight line : [Y = Mx + B ] to derive the tangent in a non-linear dataset i.e the slope of the secant line as height(h) approcahes 0.

Gradient descent is used on a Linear regression problem to minimize the cost function (J) or the MSE. The slope of the secant line(also known as derivates) describes the change in the cost function(J) with respect to the gradient(M) and intercept(B).

The learning rate is used to help the algrothim converge faster while being minimized.
